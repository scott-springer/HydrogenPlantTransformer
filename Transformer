{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"hLBUDUEgqeuB","executionInfo":{"status":"ok","timestamp":1670520698810,"user_tz":420,"elapsed":16785,"user":{"displayName":"Jiwei Yao","userId":"13030184443101343713"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"713c5968-4000-4fc7-ab90-77ae90232bb4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import math\n","from typing import Tuple\n","\n","import torch\n","from torch import nn, Tensor\n","import torch.nn.functional as F\n","from torch.nn import TransformerEncoder, TransformerEncoderLayer\n","from torch.utils.data import dataset\n","import os\n","import scipy.io\n","import pandas as pd"],"metadata":{"id":"C5U2fCve5iJG","executionInfo":{"status":"ok","timestamp":1670520682039,"user_tz":420,"elapsed":8116,"user":{"displayName":"Jiwei Yao","userId":"13030184443101343713"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["data = scipy.io.loadmat('/content/drive/MyDrive/Deep Learning/Deep Learning Project/big_data_cont.mat')"],"metadata":{"id":"QJQyFV7m5uAP","executionInfo":{"status":"ok","timestamp":1670520701458,"user_tz":420,"elapsed":2654,"user":{"displayName":"Jiwei Yao","userId":"13030184443101343713"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["class PositionEnconder(nn.Module):\n","  \"\"\"\n","  \"Since our model contains no recurrence and no convolution, in order for the \n","    model to make use of the order of the sequence, we must inject some \n","    information about the relative or absolute position of the tokens in the \n","    sequence.\" (Vaswani et al, 2017)\n","    Adapted from: \n","    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n","  \"\"\"\n","  def __init__(self, d_model:int=512, batch_first:bool=True, dropout:float=0.5, max_len:int=512):\n","    super().__init__()\n","\n","    self.dropout = nn.Dropout(p=dropout)\n","\n","    self.len_dim = 1 if batch_first else 0\n","\n","    # copy pasted from PyTorch tutorial\n","    position = torch.arange(max_len).unsqueeze(1)\n","    \n","    div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n","    \n","    pe = torch.zeros(max_len, 1, d_model)\n","    \n","    pe[:, 0, 0::2] = torch.sin(position * div_term)\n","    \n","    pe[:, 0, 1::2] = torch.cos(position * div_term)\n","    \n","    self.register_buffer('pe', pe)\n","\n","  def forward(self, x:Tensor) -> Tensor:\n","    \n","    x = x + self.pe[:x.size(self.x_dim)]\n","\n","    return x"],"metadata":{"id":"7-xr8ZxwA9hB","executionInfo":{"status":"ok","timestamp":1670520747375,"user_tz":420,"elapsed":3,"user":{"displayName":"Jiwei Yao","userId":"13030184443101343713"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["class time_series_transformer(nn.Module):\n","  def __init__(self, input_size:int, input_seq_len:int, output_size:int, output_seq_len:int, batch_first:bool,\n","               dim_val:int, encoder_layers:int, decoder_layers:int, n_head:int, dropout_pos_encoder:float, dropout_encoder:float,\n","               dropout_decoder:float, dim_feedforward_encoder: int, dim_feedforward_decoder: int):\n","    super().__init__()\n","    \n","    self.encoder_input_layer = nn.Linear(input_size, dim_val)\n","\n","    self.pos_encoder = PositionEnconder(d_model=dim_val, dropout=dropout_pos_encoder, max_len=input_seq_len)\n","\n","    self.encoder_layer = nn.TransformerEncoderLayer(dim_val, n_head, dim_feedforward_encoder, dropout_encoder, batch_first=batch_first)\n","\n","    self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=encoder_layers)\n","\n","    self.output_linear_mapping = nn.Linear(dim_val, output_size)\n","\n","  def forward(self, x):\n","\n","    out = self.encoder_input_layer(x)\n","\n","    out = self.pos_encoder(out)\n","\n","    out = self.encoder(out)\n","\n","    out = self.output_linear_mapping(out)\n","\n","    return out"],"metadata":{"id":"1LcjugSlILbh","executionInfo":{"status":"ok","timestamp":1670520748985,"user_tz":420,"elapsed":161,"user":{"displayName":"Jiwei Yao","userId":"13030184443101343713"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def building_train_test_set(dataset, window):\n","  dataNP = data['big_data'] #takes dictionary value and numpy array\n","  df = pd.DataFrame(dataNP, columns = ['T_Cin','T_Ain','m_Ain', 'm_Cin', 'c_H2in', 'j_avg', 'T_out', 'conv', 'U'])\n","  df = df.iloc[::6, :] #convert to mintute timesteps - 10 seconds are just too fine\n","\n","  size = len(df)\n","  train_size, valid_size, test_size = int(0.7*trainsize)\n","  dataPDtrain = pd.DataFrame(dataNP[0:trainsize,:], columns = ['T_Cin','T_Ain','m_Ain', 'm_Cin', 'c_H2in', 'j_avg', 'T_out', 'conv', 'U'])\n","  dataPDvalid = pd.\n","  dataPDtest = pd.DataFrame(dataNP[trainsize:,:], columns = ['T_Cin','T_Ain','m_Ain', 'm_Cin', 'c_H2in', 'j_avg', 'T_out', 'conv', 'U'])\n","  dataPDtrain = dataPDtrain.iloc[::6, :] #convert to mintute timesteps - 10 seconds are just too fine\n","  dataPDtest = dataPDtest.iloc[::6, :]  #convert to minute timesteps - 10 seconds are just too fine\n","\n","  scaler1 = MinMaxScaler(feature_range = (-1,1)) #scale between -1 and 1\n","  scaler2 = MinMaxScaler(feature_range = (-1,1)) #scale between -1 and 1\n","\n","  #training data\n","  Xs = scaler1.fit_transform(dataPDtrain[['T_Cin','T_Ain','m_Ain', 'm_Cin', 'c_H2in', 'j_avg', 'T_out', 'conv', 'U']]) #our inputs are the MVs and CVs\n","  Ys = scaler2.fit_transform(dataPDtrain[['T_out', 'conv', 'U']]) #our inputs are the CVs - what we want to predict\n","\n","  #testing data\n","  Xst = scaler1.fit_transform(dataPDtest[['T_Cin','T_Ain','m_Ain', 'm_Cin', 'c_H2in', 'j_avg', 'T_out', 'conv', 'U']]) #our inputs are the MVs and CVs\n","  Yst = scaler2.fit_transform(dataPDtest[['T_out', 'conv', 'U']]) #our inputs are the CVs - what we want to predict\n","\n"],"metadata":{"id":"FDNAgPsTLWtF"},"execution_count":null,"outputs":[]}]}